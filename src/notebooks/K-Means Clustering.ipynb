{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering\n",
    "\n",
    "Often, the data you encounter in the real world won’t be sorted into categories and won’t have labeled answers to your question. Finding patterns in this type of data, unlabeled data, is a common theme in many machine learning applications. Unsupervised Learning is how we find patterns and structure in these data.\n",
    "\n",
    "Clustering is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or clusters. Examples of clustering applications are:\n",
    "\n",
    "- **Recommendation engines**: group products to personalize the user experience\n",
    "- **Search engines**: group news topics and search results\n",
    "- **Market segmentation**: group customers based on geography, demography, and behaviors\n",
    "- **Image segmentation**: medical imaging or road scene segmentation on self-driving cars\n",
    "- **Text clustering**: group similar texts together based on word usage\n",
    "\n",
    "The Iris data set is a famous example of unlabeled data. It consists of measurements of sepals and petals on 50 different iris flowers. Here you can see a visualization of this data set that shows how the flowers naturally form three distinct clusters. We’ll learn how to find those clusters in this lesson.\n",
    "\n",
    "![The image for this exercise will be replaced with a visualization of the iris data set.](https://content.codecademy.com/programs/machine-learning/k-means/k_means_clustering.gif)\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. So two questions arise:\n",
    "\n",
    "- How many groups do we choose?\n",
    "- How do we define similarity?\n",
    "\n",
    "k-means is the most popular and well-known clustering algorithm, and it tries to address these two questions.\n",
    "\n",
    "The `k` refers to the number of clusters (groups) we expect to find in a dataset.\n",
    "The `Means` refers to the average distance of data to each cluster center, also known as the centroid, which we are trying to minimize.\n",
    "\n",
    "It is an iterative approach:\n",
    "\n",
    "1. Place k random centroids for the initial clusters.\n",
    "2. Assign data samples to the nearest centroid.\n",
    "3. Calculate new centroids based on the above-assigned data samples.\n",
    "4. Repeat Steps 2 and 3 until convergence.\n",
    "\n",
    "Convergence occurs when points don’t move between clusters and centroids stabilize. This iterative process of updating clusters and centroids is called training.\n",
    "\n",
    "Once we are happy with our clusters, we can take a new unlabeled datapoint and quickly assign it to the appropriate cluster. This is called inference.\n",
    "\n",
    "In practice it can be tricky to know how many clusters to look for. In the example here, the algorithm is sorting the data into k=2 clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset\n",
    "\n",
    "Before we implement the k-means algorithm, let’s find a dataset. The sklearn package embeds some datasets and sample images. One of them is the Iris dataset.\n",
    "\n",
    "The Iris dataset consists of measurements of sepals and petals of 3 different plant species:\n",
    "\n",
    "Iris setosa\n",
    "Iris versicolor\n",
    "Iris virginica\n",
    "\n",
    "![Iris](https://content.codecademy.com/programs/machine-learning/k-means/iris.svg)\n",
    "\n",
    "The sepal is the part that encases and protects the flower when it is in the bud stage. A petal is a leaflike part that is often colorful.\n",
    "\n",
    "From sklearn library, import the datasets module:\n",
    "\n",
    "from sklearn import datasets\n",
    "To load the Iris dataset:\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "The Iris dataset looks like:\n",
    "\n",
    "[[ 5.1  3.5  1.4  0.2 ]\n",
    " [ 4.9  3.   1.4  0.2 ]\n",
    " [ 4.7  3.2  1.3  0.2 ]\n",
    " [ 4.6  3.1  1.5  0.2 ]\n",
    "   . . .\n",
    " [ 5.9  3.   5.1  1.8 ]]\n",
    "We call each row of data a sample. For example, each flower is one sample.\n",
    "\n",
    "Each characteristic we are interested in is a feature. For example, petal length is a feature of this dataset.\n",
    "\n",
    "The features of the dataset are:\n",
    "\n",
    "Column 0: Sepal length\n",
    "Column 1: Sepal width\n",
    "Column 2: Petal length\n",
    "Column 3: Petal width\n",
    "The 3 species of Iris plants are what we are going to cluster later in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[5.1 3.5 1.4 0.2] 0\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data)\n",
    "print(iris.target)\n",
    "print(iris.data[0, :], iris.target[0])\n",
    "print(iris.DESCR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Before K-Means\n",
    "\n",
    "To get a better sense of the data in the `iris.data` matrix, let’s visualize it!\n",
    "\n",
    "With Matplotlib, we can create a 2D scatter plot of the Iris dataset using two of its features (sepal length vs. petal length). Of course there are four different features that we could plot, but it’s much easier to visualize only two dimensions.\n",
    "\n",
    "The sepal length measurements are stored in column 0 of the matrix, and the petal length measurements are stored in column 2 of the matrix.\n",
    "\n",
    "But how do we get these values?\n",
    "\n",
    "Suppose we only want to retrieve the values that are in column 0 of a matrix, we can use the NumPy/pandas notation `[:,0]` like so:\n",
    "\n",
    "```python\n",
    "matrix[:,0]\n",
    "```\n",
    "\n",
    "`[:,0]` can be translated to `[all_rows , column_0]`\n",
    "\n",
    "Once you have the measurements we need, we can make a scatter plot like this:\n",
    "\n",
    "```python\n",
    "plt.scatter(x, y)\n",
    "```\n",
    "\n",
    "To show the plot:\n",
    "\n",
    "```python\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store iris.data\n",
    "samples = iris.data\n",
    "\n",
    "# Create x and y\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "# Plot x and y\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means: Step 1\n",
    "The K-Means algorithm:\n",
    "\n",
    "Place k random centroids for the initial clusters.\n",
    "Assign data samples to the nearest centroid.\n",
    "Update centroids based on the above-assigned data samples.\n",
    "Repeat Steps 2 and 3 until convergence.\n",
    "After looking at the scatter plot and having a better understanding of the Iris data, let’s start implementing the k-means algorithm.\n",
    "\n",
    "In this exercise, we will implement Step 1.\n",
    "\n",
    "Because we expect there to be three clusters (for the three species of flowers), let’s implement k-means where the k is 3. In real-life situations you won’t always know how many clusters to look for. We’ll learn more about how to choose k later.\n",
    "\n",
    "Using the NumPy library, we will create three random initial centroids and plot them along with our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "combined = np.array(list(zip(x, y)))\n",
    "\n",
    "# Number of clusters\n",
    "k = 3\n",
    "\n",
    "# Create x coordinates of k random centroids\n",
    "centroids_x = np.random.uniform(min(x), max(x), size=k)\n",
    "\n",
    "# Create y coordinates of k random centroids\n",
    "centroids_y = np.random.uniform(min(y), max(y), size=k)\n",
    "\n",
    "# Create centroids array\n",
    "centroids = np.array(list(zip(centroids_x, centroids_y)))\n",
    "\n",
    "print(centroids)\n",
    "\n",
    "# Make a scatter plot of x, y\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "\n",
    "# Make a scatter plot of the centroids\n",
    "plt.scatter(centroids_x, centroids_y)\n",
    "\n",
    "# Display plot\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means: Step 2\n",
    "The k-means algorithm:\n",
    "\n",
    "Place k random centroids for the initial clusters.\n",
    "Assign data samples to the nearest centroid.\n",
    "Update centroids based on the above-assigned data samples.\n",
    "Repeat Steps 2 and 3 until convergence.\n",
    "In this exercise, we will implement Step 2.\n",
    "\n",
    "Now we have the three random centroids. Let’s assign data points to their nearest centroids.\n",
    "\n",
    "To do this we’re going to use a distance formula to write a distance() function.\n",
    "\n",
    "There are many different kinds of distance formulas. The one you’re probably most familiar with is called Euclidean distance. To find the Euclidean distance between two points on a 2-d plane, make a right triangle so that the hypotenuse connects the points. The distance between them is the length of the hypotenuse.\n",
    "\n",
    "Another common distance formula is the taxicab distance. The taxicab distance between two points on a 2-d plane is the distance you would travel if you took the long way around the right triangle via the two shorter sides, just like a taxicab would have to do if it wanted to travel to the opposite corner of a city block.\n",
    "\n",
    "Different distance formulas are useful in different situations. If you’re curious, you can learn more about various distance formulas here. For this lesson, we’ll use Euclidean distance.\n",
    "\n",
    "After we write the distance() function, we are going to iterate through our data samples and compute the distance from each data point to each of the 3 centroids.\n",
    "\n",
    "Suppose we have a point and a list of three distances in distances and it looks like [15, 20, 5], then we would want to assign the data point to the 3rd centroid. The argmin(distances) would return the index of the lowest corresponding distance, 2, because the index 2 contains the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_length_width = np.array(list(zip(x, y)))\n",
    "\n",
    "# Step 1: Place K random centroids\n",
    "k = 3\n",
    "\n",
    "centroids_x = np.random.uniform(min(x), max(x), size=k)\n",
    "centroids_y = np.random.uniform(min(y), max(y), size=k)\n",
    "\n",
    "centroids = np.array(list(zip(centroids_x, centroids_y)))\n",
    "\n",
    "# Step 2: Assign samples to nearest centroid\n",
    "# Distance formula\n",
    "def distance(a, b):\n",
    "  one = (a[0] - b[0]) ** 2\n",
    "  two = (a[1] - b[1]) ** 2\n",
    "  distance = (one+two) ** 0.5\n",
    "  return distance\n",
    "\n",
    "# Cluster labels for each point (either 0, 1, or 2)\n",
    "labels = np.zeros(len(samples))\n",
    "\n",
    "# A function that assigns the nearest centroid to a sample\n",
    "def assign_to_centroid(sample, centroids):\n",
    "  k = len(centroids)\n",
    "  distances = np.zeros(k)\n",
    "  for i in range(k):\n",
    "    distances[i] = distance(sample, centroids[i])\n",
    "  closest_centroid = np.argmin(distances)\n",
    "  return closest_centroid\n",
    "\n",
    "# Assign the nearest centroid to each sample\n",
    "for i in range(len(samples)):\n",
    "  labels[i] = assign_to_centroid(samples[i], centroids)\n",
    "\n",
    "# Print labels\n",
    "print(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means: Step 3\n",
    "The k-means algorithm:\n",
    "\n",
    "Place k random centroids for the initial clusters.\n",
    "Assign data samples to the nearest centroid.\n",
    "Update centroids based on the above-assigned data samples.\n",
    "Repeat Steps 2 and 3 until convergence.\n",
    "In this exercise, we will implement Step 3.\n",
    "\n",
    "Find new cluster centers by taking the average of the assigned points. To find the average of the assigned points, we can use the .mean() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "sepal_length_width = np.array(list(zip(x, y)))\n",
    "\n",
    "# Step 1: Place K random centroids\n",
    "\n",
    "k = 3\n",
    "\n",
    "centroids_x = np.random.uniform(min(x), max(x), size=k)\n",
    "centroids_y = np.random.uniform(min(y), max(y), size=k)\n",
    "\n",
    "centroids = np.array(list(zip(centroids_x, centroids_y)))\n",
    "\n",
    "# Step 2: Assign samples to nearest centroid\n",
    "\n",
    "# Distance formula\n",
    "\n",
    "def distance(a, b):\n",
    "  one = (a[0] - b[0]) **2\n",
    "  two = (a[1] - b[1]) **2\n",
    "  distance = (one+two) ** 0.5\n",
    "  return distance\n",
    "\n",
    "# Cluster labels for each point (either 0, 1, or 2)\n",
    "\n",
    "labels = np.zeros(len(samples))\n",
    "\n",
    "# A function that assigns the nearest centroid to a sample\n",
    "\n",
    "def assign_to_centroid(sample, centroids):\n",
    "  k = len(centroids)\n",
    "  distances = np.zeros(k)\n",
    "  for i in range(k):\n",
    "    distances[i] = distance(sample, centroids[i])\n",
    "  closest_centroid = np.argmin(distances)\n",
    "  return closest_centroid\n",
    "\n",
    "# Assign the nearest centroid to each sample\n",
    "\n",
    "for i in range(len(samples)):\n",
    "  labels[i] = assign_to_centroid(samples[i], centroids)\n",
    "\n",
    "# Step 3: Update centroids\n",
    "\n",
    "centroids_old = deepcopy(centroids)\n",
    "\n",
    "for i in range(k):\n",
    "  points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]\n",
    "  centroids[i] = np.mean(points, axis=0)\n",
    "  \n",
    "print(centroids_old)\n",
    "print(\"- - - - - - - - - - - - - -\")\n",
    "print(centroids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means: Step 4\n",
    "The k-means algorithm:\n",
    "\n",
    "Place k random centroids for the initial clusters.\n",
    "Assign data samples to the nearest centroid.\n",
    "Update centroids based on the above-assigned data samples.\n",
    "Repeat Steps 2 and 3 until convergence.\n",
    "In this exercise, we will implement Step 4.\n",
    "\n",
    "This is the part of the algorithm where we repeatedly execute Step 2 and 3 until the centroids stabilize (convergence).\n",
    "\n",
    "We can do this using a while loop. And everything from Step 2 and 3 goes inside the loop.\n",
    "\n",
    "For the condition of the while loop, we need to create an array named errors. In each error index, we calculate the difference between the updated centroid (centroids) and the old centroid (centroids_old).\n",
    "\n",
    "The loop ends when all three values in errors are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "sepal_length_width = np.array(list(zip(x, y)))\n",
    "\n",
    "# Step 1: Place K random centroids\n",
    "\n",
    "k = 3\n",
    "\n",
    "centroids_x = np.random.uniform(min(x), max(x), size=k)\n",
    "centroids_y = np.random.uniform(min(y), max(y), size=k)\n",
    "\n",
    "centroids = np.array(list(zip(centroids_x, centroids_y)))\n",
    "\n",
    "def distance(a, b):\n",
    "  one = (a[0] - b[0]) ** 2\n",
    "  two = (a[1] - b[1]) ** 2\n",
    "  distance = (one + two) ** 0.5\n",
    "  return distance\n",
    "\n",
    "# A function that assigns the nearest centroid to a sample\n",
    "def assign_to_centroid(sample, centroids):\n",
    "  k = len(centroids)\n",
    "  distances = np.zeros(k)\n",
    "  for i in range(k):\n",
    "    distances[i] = distance(sample, centroids[i])\n",
    "  closest_centroid = np.argmin(distances)\n",
    "  return closest_centroid\n",
    "\n",
    "# To store the value of centroids when it updates\n",
    "centroids_old = np.zeros(centroids.shape)\n",
    "\n",
    "# Cluster labeles (either 0, 1, or 2)\n",
    "labels = np.zeros(len(samples))\n",
    "\n",
    "distances = np.zeros(3)\n",
    "\n",
    "# Initialize error:\n",
    "error = np.zeros(3)\n",
    "\n",
    "for i in range(k):\n",
    "  error[i] = distance(centroids[i], centroids_old[i])\n",
    "\n",
    "# Repeat Steps 2 and 3 until convergence:\n",
    "\n",
    "while error.all() != 0:\n",
    "\n",
    "  # Step 2: Assign samples to nearest centroid\n",
    "\n",
    "  for i in range(len(samples)):\n",
    "    labels[i] = assign_to_centroid(samples[i], centroids)\n",
    "\n",
    "  # Step 3: Update centroids\n",
    "\n",
    "  centroids_old = deepcopy(centroids)\n",
    "\n",
    "  for i in range(k):\n",
    "    points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]\n",
    "    centroids[i] = np.mean(points, axis=0)\n",
    "    error[i] = distance(centroids[i], centroids_old[i])\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for i in range(k):\n",
    "  points = np.array([sepal_length_width[j] for j in range(len(samples)) if labels[j] == i])\n",
    "  plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.5)\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=150)\n",
    "\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means: Scikit-Learn\n",
    "Awesome, you have implemented k-means clustering from scratch!\n",
    "\n",
    "Writing an algorithm whenever you need it can be very time-consuming and you might make mistakes and typos along the way. We will now show you how to implement k-means more efficiently – using the scikit-learn library.\n",
    "\n",
    "There are many advantages to using scikit-learn. It can run k-means on datasets with as many features as your computer can handle, so it will be easy for us to use all four features of the iris data set instead of the two features that we used in the previous exercises.\n",
    "\n",
    "Another big advantage of scikit-learn is that it is a widely-used open-source library. It is very well-tested, so it is much less likely to contain mistakes. Since so many people use it, there are many online resources that can help you if you get stuck. If you have a specific question about scikit-learn, it’s very likely that other users have already asked and answered your question on public forums.\n",
    "\n",
    "To import KMeans from sklearn.cluster:\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "For Step 1, use the KMeans() method to build a model that finds k clusters. To specify the number of clusters (k), use the n_clusters keyword argument:\n",
    "\n",
    "model = KMeans(n_clusters = k)\n",
    "For Steps 2 and 3, use the .fit() method to compute k-means clustering:\n",
    "\n",
    "model.fit(X)\n",
    "After k-means, we can now predict the closest cluster each sample in X belongs to. Use the .predict() method to compute cluster centers and predict cluster index for each sample:\n",
    "\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "# Use KMeans() to create a model that finds 3 clusters\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use .fit() to fit the model to samples\n",
    "model.fit(samples)\n",
    "\n",
    "# Use .predict() to determine the labels of samples \n",
    "labels = model.predict(samples)\n",
    "\n",
    "# Print the labels\n",
    "print(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data?\n",
    "You used k-means and found three clusters of the samples data. But it gets cooler!\n",
    "\n",
    "Since you have created a model that computed k-means clustering, you can now feed new data samples into it and obtain the cluster labels using the .predict() method.\n",
    "\n",
    "So, suppose we went to the florist and bought 3 more Irises with the measurements:\n",
    "\n",
    "[[ 5.1  3.5  1.4  0.2 ]\n",
    " [ 3.4  3.1  1.6  0.3 ]\n",
    " [ 4.9  3.   1.4  0.2 ]]\n",
    "We can feed this new data into the model and obtain the labels for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "# Store the new Iris measurements\n",
    "new_samples = np.array([[5.7, 4.4, 1.5, 0.4],\n",
    "   [6.5, 3. , 5.5, 0.4],\n",
    "   [5.8, 2.7, 5.1, 1.9]])\n",
    "\n",
    "# Predict labels for the new_samples\n",
    "new_labels = model.predict(new_samples)\n",
    "\n",
    "print(new_labels)\n",
    "\n",
    "new_names = [iris.target_names[label] for label in new_labels]\n",
    "\n",
    "print(new_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize After K-Means\n",
    "\n",
    "We have done the following using sklearn library:\n",
    "\n",
    "Load the embedded dataset\n",
    "Compute k-means on the dataset (where k is 3)\n",
    "Predict the labels of the data samples\n",
    "And the labels resulted in either 0, 1, or 2.\n",
    "\n",
    "Let’s finish it by making a scatter plot of the data again!\n",
    "\n",
    "This time, however, use the labels numbers as the colors.\n",
    "\n",
    "To edit colors of the scatter plot, we can set c = labels:\n",
    "\n",
    "plt.scatter(x, y, c=labels, alpha=0.5)\n",
    " \n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "labels = model.predict(samples)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "# Make a scatter plot of x and y and using labels to define the colors\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "plt.scatter(x, y, c=labels, alpha=0.5)\n",
    " \n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "At this point, we have clustered the Iris data into 3 different groups (implemented using Python and using scikit-learn). But do the clusters correspond to the actual species? Let’s find out!\n",
    "\n",
    "First, remember that the Iris dataset comes with target values:\n",
    "\n",
    "target = iris.target\n",
    "It looks like:\n",
    "\n",
    "[ 0 0 0 0 0 ... 2 2 2]\n",
    "According to the metadata:\n",
    "\n",
    "All the 0‘s are Iris-setosa\n",
    "All the 1‘s are Iris-versicolor\n",
    "All the 2‘s are Iris-virginica\n",
    "Let’s change these values into the corresponding species using the following code:\n",
    "\n",
    "species = [iris.target_names[t] for t in list(target)]\n",
    "Then we are going to use the Pandas library to perform a cross-tabulation.\n",
    "\n",
    "Cross-tabulations enable you to examine relationships within the data that might not be readily apparent when analyzing total survey responses.\n",
    "\n",
    "The result should look something like:\n",
    "\n",
    "labels    setosa    versicolor    virginica\n",
    "0             50             0            0\n",
    "1              0             2           36\n",
    "2              0            48           14\n",
    "(You might need to expand this narrative panel in order to the read the table better.)\n",
    "\n",
    "The first column has the cluster labels. The second to fourth columns have the Iris species that are clustered into each of the labels.\n",
    "\n",
    "By looking at this, you can conclude that:\n",
    "\n",
    "Iris-setosa was clustered with 100% accuracy.\n",
    "Iris-versicolor was clustered with 96% accuracy.\n",
    "Iris-virginica didn’t do so well.\n",
    "Follow the instructions below to learn how to do a cross-tabulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "target = iris.target\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "labels = [iris.target_names[s] for s in model.predict(samples)]\n",
    "\n",
    "# Code starts here:\n",
    "species = [iris.target_names[t] for t in list(target)]\n",
    "\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    " \n",
    "print(df)\n",
    "\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "print(ct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of Clusters\n",
    "\n",
    "At this point, we have grouped the Iris plants into 3 clusters. But suppose we didn’t know there are three species of Iris in the dataset, what is the best number of clusters? And how do we determine that?\n",
    "\n",
    "Before we answer that, we need to define what is a good cluster?\n",
    "\n",
    "Good clustering results in tight clusters, meaning that the samples in each cluster are bunched together. How spread out the clusters are is measured by inertia. Inertia is the distance from each sample to the centroid of its cluster. The lower the inertia is, the better our model has done.\n",
    "\n",
    "You can check the inertia of a model by:\n",
    "\n",
    "```python\n",
    "print(model.inertia_)\n",
    "```\n",
    "\n",
    "For the Iris dataset, if we graph all the ks (number of clusters) with their inertias:\n",
    "\n",
    "![Optimal Number of Clusters](https://content.codecademy.com/programs/machine-learning/k-means/number-of-clusters.svg)\n",
    "\n",
    "Notice how the graph keeps decreasing.\n",
    "\n",
    "Ultimately, this will always be a trade-off. If the inertia is too large, then the clusters probably aren’t clumped close together. On the other hand, if there are too many clusters, the individual clusters might not be different enough from each other. The goal is to have low inertia and a small number of clusters.\n",
    "\n",
    "One of the ways to interpret this graph is to use the elbow method: choose an “elbow” in the inertia plot - when inertia begins to decrease more slowly.\n",
    "\n",
    "In the graph above, 3 is the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "# Code Start here:\n",
    "num_clusters = [1,2,3,4,5,6,7,8,]\n",
    "inertias = []\n",
    "\n",
    "for i in num_clusters:\n",
    "  model = KMeans(n_clusters = i)\n",
    "  model.fit(samples)\n",
    "  inertias.append(model.inertia_)\n",
    "\n",
    "plt.plot(num_clusters, inertias, '-o')\n",
    " \n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try It On Your Own\n",
    "Now it is your turn!\n",
    "\n",
    "In this review section, find another dataset from one of the following:\n",
    "\n",
    "The scikit-learn library\n",
    "UCI Machine Learning Repo\n",
    "Codecademy GitHub Repo (coming soon!)\n",
    "Import the pandas library as pd:\n",
    "\n",
    "import pandas as pd\n",
    "Load in the data with read_csv():\n",
    "\n",
    "digits = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\", header=None)\n",
    "Note that if you download the data like this, the data is already split up into a training and a test set, indicated by the extensions .tra and .tes. You’ll need to load in both files.\n",
    "\n",
    "With the command above, you only load in the training set.\n",
    "\n",
    "Happy Coding!\n",
    "\n",
    "Instructions\n",
    "Implement k-means clustering on another dataset and see what you can find. Here are some questions to get you started.\n",
    "\n",
    "How does the model perform?\n",
    "How did you choose the number of clusters?\n",
    "If you think you found something interesting, let us know by posting it on Facebook, Twitter, or Instagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "sepal_length_width = np.array(list(zip(x, y)))\n",
    "\n",
    "# Step 1: Place K random centroids\n",
    "\n",
    "k = 3\n",
    "\n",
    "centroids_x = np.random.uniform(min(x), max(x), size=k)\n",
    "centroids_y = np.random.uniform(min(y), max(y), size=k)\n",
    "\n",
    "centroids = np.array(list(zip(centroids_x, centroids_y)))\n",
    "\n",
    "# Step 2: Assign samples to nearest centroid\n",
    "# Distance formula\n",
    "def distance(a, b):\n",
    "  one = (a[0] - b[0]) ** 2\n",
    "  two = (a[1] - b[1]) ** 2\n",
    "  distance = (one+two) ** 0.5\n",
    "  return distance\n",
    "\n",
    "# Cluster labels for each point (either 0, 1, or 2)\n",
    "labels = np.zeros(len(samples))\n",
    "\n",
    "# A function that assigns the nearest centroid to a sample\n",
    "def assign_to_centroid(sample, centroids):\n",
    "  k = len(centroids)\n",
    "  distances = np.zeros(k)\n",
    "  for i in range(k):\n",
    "    distances[i] = distance(sample, centroids[i])\n",
    "  closest_centroid = np.argmin(distances)\n",
    "  return closest_centroid\n",
    "\n",
    "# Assign the nearest centroid to each sample\n",
    "for i in range(len(samples)):\n",
    "  labels[i] = assign_to_centroid(samples[i], centroids)\n",
    "\n",
    "# Print labels\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
