{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "subject: Logistic Regression\n",
    "title: Predict Credit Card Fraud\n",
    "path: Natural Language Processing Specialist\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit card fraud is one of the leading causes of identify theft around the world. In 2018 alone, over $24 billion were stolen through fraudulent credit card transactions. Financial institutions employ a wide variety of different techniques to prevent fraud, one of the most common being Logistic Regression.\n",
    "\n",
    "In this project, you are a Data Scientist working for a credit card company. You have access to a dataset (based on a synthetic financial dataset), that represents a typical set of credit card transactions. `transactions.csv` is the original dataset containing 200k transactions. For starters, we're going to be working with a small portion of this dataset, `transactions_modified.csv`, which contains one thousand transactions. Your task is to use Logistic Regression and create a predictive model to determine if a transaction is fraudulent or not.\n",
    "\n",
    "Note that a `solution.py` file is loaded for you in the workspace, which contains solution code for this project. We highly recommend that you complete the project on your own without checking the solution, but feel free to take a look if you get stuck or want to check your answers when you're done!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. The file `transactions_modified.csv` contains data on 1000 simulated credit card transactions. Let's begin by loading the data into a pandas DataFrame named transactions. Take a peek at the dataset using `.head()` and you can use `.info()` to examine how many rows are there and what datatypes the are. How many transactions are fraudulent? Print your answer.\n",
    "\n",
    "    The isFraud column gives information on fraud versus not with 1 representing a fraudulent transaction and 0 representing non-fraudulent transaction. You can use the `.sum()` method to add the rows and the number you get will be the number of fraudulent transactions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0             0.0        0               0  \n",
      "1  M2044282225             0.0             0.0        0               0  \n",
      "2   C553264065             0.0             0.0        1               0  \n",
      "3    C38997010         21182.0             0.0        1               0  \n",
      "4  M1230701703             0.0             0.0        0               0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6362620 entries, 0 to 6362619\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   step            int64  \n",
      " 1   type            object \n",
      " 2   amount          float64\n",
      " 3   nameOrig        object \n",
      " 4   oldbalanceOrg   float64\n",
      " 5   newbalanceOrig  float64\n",
      " 6   nameDest        object \n",
      " 7   oldbalanceDest  float64\n",
      " 8   newbalanceDest  float64\n",
      " 9   isFraud         int64  \n",
      " 10  isFlaggedFraud  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 534.0+ MB\n",
      "None\n",
      "Number of fraudulent transactions: 8213\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "transactions = pd.read_csv('D:/Repositories/data/transactions.csv')\n",
    "\n",
    "# View the first few rows of the dataset\n",
    "print(transactions.head())\n",
    "\n",
    "# View the summary information of the dataset\n",
    "print(transactions.info())\n",
    "\n",
    "fraudulent_transactions = transactions['isFraud'].sum()\n",
    "print(\"Number of fraudulent transactions:\", fraudulent_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many fraudulent transactions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "2. Looking at the dataset, combined with our knowledge of credit card transactions in general, we can see that there are a few interesting columns to look at. We know that the amount of a given transaction is going to be important. Calculate summary statistics for this column. What does the distribution look like?\n",
    "\n",
    "    Use the `.describe()` on a column like this:\n",
    "    \n",
    "    ```python\n",
    "    pd['column'].describe()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    6.362620e+06\n",
      "mean     1.798619e+05\n",
      "std      6.038582e+05\n",
      "min      0.000000e+00\n",
      "25%      1.338957e+04\n",
      "50%      7.487194e+04\n",
      "75%      2.087215e+05\n",
      "max      9.244552e+07\n",
      "Name: amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics on amount column\n",
    "amount_stats = transactions['amount'].describe()\n",
    "print(amount_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We have a lot of information about the type of transaction we are looking at. Let's create a new column called `isPayment` that assigns a 1 when type is “PAYMENT” or “DEBIT”, and a 0 otherwise.\n",
    "\n",
    "    You can create a new column for a pandas DataFrame like this:\n",
    "\n",
    "    ```python\n",
    "    df['new_column'] = value\n",
    "    ```\n",
    "\n",
    "    You can filter a DataFrame for specific values like this:\n",
    "\n",
    "    ```python\n",
    "    df[df['filter_column'] == value]\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \\\n",
      "0  M1979787155             0.0             0.0        0               0   \n",
      "1  M2044282225             0.0             0.0        0               0   \n",
      "2   C553264065             0.0             0.0        1               0   \n",
      "3    C38997010         21182.0             0.0        1               0   \n",
      "4  M1230701703             0.0             0.0        0               0   \n",
      "\n",
      "   isPayment  \n",
      "0       True  \n",
      "1       True  \n",
      "2      False  \n",
      "3      False  \n",
      "4       True  \n"
     ]
    }
   ],
   "source": [
    "# Create isPayment field\n",
    "transactions['isPayment'] = (transactions['type'] == 'PAYMENT') | (transactions['type'] == 'DEBIT')\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Similarly, create a column called `isMovement`, which will capture if money moved out of the origin account. This column will have a value of 1 when type is either `CASH_OUT` or `TRANSFER`, and a 0 otherwise.\n",
    "\n",
    "    You can create a new column for a pandas DataFrame like this:\n",
    "\n",
    "    ```python\n",
    "    df['new_column'] = value\n",
    "    ```\n",
    "\n",
    "    You can filter a DataFrame for specific values like this:\n",
    "\n",
    "    ```python\n",
    "    df[df['filter_column'] == value]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \\\n",
      "0  M1979787155             0.0             0.0        0               0   \n",
      "1  M2044282225             0.0             0.0        0               0   \n",
      "2   C553264065             0.0             0.0        1               0   \n",
      "3    C38997010         21182.0             0.0        1               0   \n",
      "4  M1230701703             0.0             0.0        0               0   \n",
      "\n",
      "   isPayment  isMovement  \n",
      "0       True       False  \n",
      "1       True       False  \n",
      "2      False        True  \n",
      "3      False        True  \n",
      "4       True       False  \n"
     ]
    }
   ],
   "source": [
    "# Create isMovement field\n",
    "transactions['isMovement'] = (transactions['type'] == 'CASH_OUT') | (transactions['type'] == 'TRANSFER')\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. With financial fraud, another key factor to investigate would be the difference in value between the origin and destination account. Our theory, in this case, being that destination accounts with a significantly different value could be suspect of fraud.\n",
    "\n",
    "   Let's create a column called `accountDiff` with the absolute difference of the `oldbalanceOrg` and `oldbalanceDest` columns.\n",
    "\n",
    "   You can perform standard mathematical functions like `+`, `-`, `*`, and `/` with entire columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \\\n",
      "0  M1979787155             0.0             0.0        0               0   \n",
      "1  M2044282225             0.0             0.0        0               0   \n",
      "2   C553264065             0.0             0.0        1               0   \n",
      "3    C38997010         21182.0             0.0        1               0   \n",
      "4  M1230701703             0.0             0.0        0               0   \n",
      "\n",
      "   isPayment  isMovement  accountDiff  \n",
      "0       True       False     170136.0  \n",
      "1       True       False      21249.0  \n",
      "2      False        True        181.0  \n",
      "3      False        True      21001.0  \n",
      "4       True       False      41554.0  \n"
     ]
    }
   ],
   "source": [
    "# Create accountDiff field\n",
    "transactions['accountDiff'] = abs(transactions['oldbalanceOrg'] - transactions['oldbalanceDest'])\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and Split the Data\n",
    "\n",
    "6. Before we can start training our model, we need to define our features and label columns. Our label column in this dataset is the isFraud field. Create a variable called features which will be an array consisting of the following fields:\n",
    "\n",
    "   - `amount`\n",
    "   - `isPayment`\n",
    "   - `isMovement`\n",
    "   - `accountDiff`\n",
    "\n",
    "    Also create a variable called `label` with the column `isFraud`.\n",
    "\n",
    "    You can assign an entire DataFrame or a pandas Series (one column) to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and label variables\n",
    "features = transactions[['amount', 'isPayment', 'isMovement', 'accountDiff']]\n",
    "label = transactions['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Split the data into training and test sets using sklearn's `train_test_split()` method. We'll use the training set to train the model and the test set to evaluate the model. Use a `test_size` value of 0.3.\n",
    "\n",
    "    To capture all of the output arrays, use the method like this:\n",
    "\n",
    "    `X_train, X_test, y_train, y_test = train_test_split(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Data\n",
    "\n",
    "8. Since sklearn‘s Logistic Regression implementation uses `Regularization`, we need to scale our feature data. Create a `StandardScaler` object, `.fit_transform()` it on the training features, and `.transform()` the test features.\n",
    "\n",
    "    Pass the entire feature variables as the argument for your `StandardScaler` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Evaluate the Model\n",
    "\n",
    "9. Create a LogisticRegression model with sklearn and `.fit()` it on the training data.\n",
    "\n",
    "    Fitting the model find the best coefficients for our selected features so it can more accurately predict our label. We will start with the default threshold of 0.5.\n",
    "\n",
    "    Pass the newly normalized training features as the argument to your `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Run the model's `.score()` method on the training data and print the training score.\n",
    "\n",
    "    Scoring the model on the training data will process the training data through the trained model and will predict which transactions are fraudulent. The score returned is the percentage of correct classifications, or the accuracy.\n",
    "\n",
    "    Pass both the training features and label variables to the `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9986829325026483\n"
     ]
    }
   ],
   "source": [
    "# Score the model on the training data\n",
    "training_score = model.score(X_train_scaled, y_train)\n",
    "print(\"Training score:\", training_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Run the model's `.score()` method on the test data and print the test score.\n",
    "\n",
    "    Scoring the model on the test data will process the test data through the trained model and will predict which transactions are fraudulent. The score returned is the percentage of correct classifications, or the accuracy, and will be an indicator for the sucess of your model.\n",
    "\n",
    "    How did your model perform?\n",
    "\n",
    "    Pass both the test features and label variables to the `.score()` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9987017926577416\n"
     ]
    }
   ],
   "source": [
    "# Score the model on the test data\n",
    "test_score = model.score(X_test_scaled, y_test)\n",
    "print(\"Test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Print the coefficients for our model to see how important each feature column was for prediction. Which feature was most important? Least important?\n",
    "\n",
    "    To print the model coefficients, use the `.coef_ method` on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      " amount         0.221755\n",
      "isPayment     -1.171221\n",
      "isMovement     3.588887\n",
      "accountDiff   -0.661338\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the model coefficients\n",
    "coefficients = model.coef_\n",
    "feature_importance = pd.Series(coefficients[0], index=features.columns)\n",
    "print(\"Feature importance:\\n\", feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict With the Model\n",
    "\n",
    "13. Let's use our model to process more transactions that have gone through our systems. There are three numpy arrays pre-loaded in the workspace with information on new sample transactions under \"New transaction data\"\n",
    "\n",
    "    ```python\n",
    "    # New transaction data\n",
    "    transaction1 = np.array([123456.78, 0.0, 1.0, 54670.1])\n",
    "    transaction2 = np.array([98765.43, 1.0, 0.0, 8524.75])\n",
    "    transaction3 = np.array([543678.31, 1.0, 0.0, 510025.5])\n",
    "    ```\n",
    "    Create a fourth array, `your_transaction`, and add any transaction information you'd like. Make sure to enter all values as floats with a `.!`\n",
    "\n",
    "    Make sure all of your array values are floats (have a decimal point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New transaction data\n",
    "tr1 = np.array([123456.78, 0.0, 1.0, 54670.1])\n",
    "tr2 = np.array([98765.43, 1.0, 0.0, 8524.75])\n",
    "tr3 = np.array([543678.31, 1.0, 0.0, 510025.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Combine the new transactions and `your_transaction` into a single numpy array called `sample_transactions`.\n",
    "\n",
    "    You can combine numpy arrays using the `.stack()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new transaction\n",
    "your_tr = np.array([354748.31, 1.0, 0.0, 30725.5])\n",
    "\n",
    "# Combine new transactions into a single array\n",
    "sample_transactions = np.stack([tr1, tr2, tr3, your_tr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Since our Logistic Regression model was trained on scaled feature data, we must also scale the feature data we are making predictions on. Using the `StandardScaler` object created earlier, apply its `.transform()` method to `sample_transactions` and save the result to `sample_transactions`.\n",
    "\n",
    "    Use the `.transform()` method with the same model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Normalize the new transactions\n",
    "# Scale the sample transactions\n",
    "sample_transactions_scaled = scaler.transform(sample_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Which transactions are fraudulent? Use your model's `.predict()` method on `sample_transactions` and print the result to find out.\n",
    "\n",
    "    Want to see the probabilities that led to these predictions? Call your model's `.predict_proba()` method on `sample_transactions` and print the result. The 1st column is the probability of a transaction not being fraudulent, and the 2nd column is the probability of a transaction being fraudulent (which was calculated by our model to make the final classification decision).\n",
    "\n",
    "    You can pass `sample_transactions` to both the `.predict()` and `.predict_proba()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0]\n",
      "Probabilities:\n",
      " [[9.96831107e-01 3.16889315e-03]\n",
      " [9.99999806e-01 1.94016916e-07]\n",
      " [9.99999790e-01 2.10487274e-07]\n",
      " [9.99999788e-01 2.12414213e-07]]\n"
     ]
    }
   ],
   "source": [
    "# Predict fraud on the new transactions\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_transactions_scaled)\n",
    "probabilities = model.predict_proba(sample_transactions_scaled)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Probabilities:\\n\", probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Congratulations on completing the project!\n",
    "\n",
    "    Note that we'd used a modified version of the dataset. You can now try to re-run the project using the original dataset, `transactions.csv`. Examine how the results change. If you notice something weird, you're totally on to something! That \"something\" is what is known as an imbalanced class classification problem.\n",
    "\n",
    "    We will cover this very relevant topic (among many other things) in the Logistic Regression II module!\n",
    "\n",
    "    Check how many fraudulent transactions are there in the complete dataset. What percentage of the total number of transactions is this? What was the this percentage in the modified dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probabilities on the new transactions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
